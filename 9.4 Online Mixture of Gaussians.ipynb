{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please consult the end of chapter 9.4 of _Pattern Recognition and Machine Learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for Online Mixture of Gaussians\n",
    "\n",
    "Online algorithms consider a single data point at a time rather than an entire batch. Online could also be called \"incremental.\" \n",
    "\n",
    "When applying Expectation Maximization to a gaussian mixture given large amounts of data, the batch method's calculation time depends on the number of data points. In the online formulation, the Expectation and Maximization steps both take fixed time since they operate on only a single data point. This can provide a significant performance boost when you don't need to consider the entire batch of data every iteration.\n",
    "\n",
    "In the online formulation of Mixture of Gaussians, the parameters are updated incrementally. That means the algorithm can converge more quickly than a batch approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T09:22:21.020401Z",
     "start_time": "2018-04-24T09:22:21.015881Z"
    }
   },
   "source": [
    "# Derivation of Online Gaussian Mixture\n",
    "\n",
    "We derive 9.78 & 9.79 in the text. The purpose is to define an incremental update step to the mean of a gaussian. We start from 9.18, the definition of the mean.\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\mu}_k &= \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma(z_{nk}) \\mathbf{x}_n \\\\\n",
    "    N_k &= \\sum_{n=1}^{N} \\gamma(z_{nk})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating N\n",
    "We will consider an update technique where we recompute the responsibilities for a single data point, $\\mathbf{x}_m$. We initialize by using our definition from above.\n",
    "\n",
    "\\begin{align}\n",
    "    N_{k}^{old} &= \\sum_{n} \\gamma^{old}(z_{nk}) \\\\\n",
    "    N_{k}^{new} &= \\sum_{n \\neq m} \\gamma^{old}(z_{nk}) + \\gamma^{new}(z_{mk})\n",
    "\\end{align}\n",
    "\n",
    "We can use these results, subtitute $N_{k}^{old}$ into the ladder equation to retrieve 9.79. (Subtract out when $m = n$ to convert $N_{k}^{old}$ to the sum in the ladder equation.)\n",
    "\n",
    "\\begin{equation}\n",
    "    N_{k}^{new} = N_{k}^{old} + \\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the mean\n",
    "Now, let us derive the mean update. We use a similar update technique.\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\mu}_k^{old} &= \\frac{1}{N_k^{old}} \\sum_{n=1}^{N} \\gamma^{old} (z_{nk}) \\mathbf{x}_n\n",
    "\\end{align}\n",
    "\n",
    "Now we recompute the responsibilities, $\\gamma(z_{mk})$, from a single point.\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\mu}_k^{new} &= \\frac{1}{N_k^{new}} \\Big(\n",
    "                            \\sum_{n \\neq m} \\gamma^{old}(z_{nk}) \\mathbf{x}_n + \\gamma^{new}(z_{mk}) \\mathbf{x}_m \\Big) \\\\\n",
    "    &= \\frac{1}{N_k^{new}}\n",
    "        \\Big( N_k^{old} \\mathbf{\\mu}_k^{old} - \\gamma^{old}(z_{mk}) \\mathbf{x}_m + \\gamma^{new}(z_{mk}) \\mathbf{x}_m \\Big) \\\\\n",
    "    &= \\frac{1}{N_k^{new}}\n",
    "        \\Big( \\big ( N_{k}^{new} - \\gamma^{new}(z_{mk}) + \\gamma^{old}(z_{mk}) \\big ) \\mathbf{\\mu}_k^{old} - \\gamma^{old}(z_{mk}) \\mathbf{x}_m + \\gamma^{new}(z_{mk}) \\mathbf{x}_m \\Big) \\\\\n",
    "    &= \\mathbf{\\mu}_k^{old} + \n",
    "        \\Big( \\frac{\\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})}{N_{k}^{new}} \\Big)\n",
    "        (\\mathbf{x}_m - \\mathbf{\\mu}_k^{old})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the covariances\n",
    "\n",
    "Similar update but for covariances\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\Sigma}_k^{old} &= \\frac{1}{N_k^{old}} \\sum_{n} \\gamma^{old} (z_{nk}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{old})\n",
    "        (\\mathbf{x}_n - \\mathbf{\\mu}_k^{old})^{T}\n",
    "\\end{align}\n",
    "\n",
    "Now we recompute the responsibilities, $\\gamma(z_{mk})$, from a single point.\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\Sigma}_k^{new} &= \\frac{1}{N_k^{new}} \\sum_{n \\neq m} \\Big(\n",
    "        \\gamma^{new} (z_{nk}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{new}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{new})^{T} \\Big) \\\\\n",
    "    &= \\frac{1}{N_k^{new}} \\sum_{n \\neq m} \\Big(\n",
    "        \\gamma^{old} (z_{nk}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{new}) (\\mathbf{x}_n - \\mathbf{\\mu}_k^{new})^{T} +\n",
    "        \\gamma^{new} (z_{mk}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new})^{T} \\Big) \\\\\n",
    "    &= \\frac{1}{N_k^{new}} \\Big(\n",
    "        N_k^{old} \\mathbf{\\Sigma}_k^{old} - \n",
    "        \\gamma^{old} (z_{mk}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{old}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{old})^{T} +\n",
    "        \\gamma^{new} (z_{mk}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new})^{T} \\Big) \\\\\n",
    "\\end{align}\n",
    "\n",
    "For space, define \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{A}^{new} &= (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new})^{T} \\\\\n",
    "    \\mathbf{A}^{old} &= (\\mathbf{x}_m - \\mathbf{\\mu}_k^{old}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{old})^{T}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\Sigma}_k^{new} &=\n",
    "        \\frac{1}{N_k^{new}} \\Big(\n",
    "        N_k^{old} \\mathbf{\\Sigma}_k^{old} - \n",
    "        \\gamma^{old} (z_{mk}) \\mathbf{A}^{old} +\n",
    "        \\gamma^{new} (z_{mk}) \\mathbf{A}^{new} \\Big) \\\\\n",
    "    &= \\frac{1}{N_k^{new}} \\Big(\n",
    "        N_{k}^{new} \\mathbf{\\Sigma}_k^{old} - \n",
    "        \\gamma^{new}(z_{mk}) \\mathbf{\\Sigma}_k^{old} + \n",
    "        \\gamma^{old}(z_{mk}) \\mathbf{\\Sigma}_k^{old} - \n",
    "        \\gamma^{old} (z_{mk}) \\mathbf{A}^{old} +\n",
    "        \\gamma^{new} (z_{mk}) \\mathbf{A}^{new} \\Big) \\\\ \n",
    "    &= \\frac{1}{N_k^{new}} \\Big(\n",
    "        N_{k}^{new} \\mathbf{\\Sigma}_k^{old} +\n",
    "        \\big ( \\gamma^{old}(z_{mk}) - \\gamma^{new}(z_{mk}) \\big ) \\mathbf{\\Sigma}_k^{old} -\n",
    "        \\gamma^{old} (z_{mk}) \\mathbf{A}^{old} +\n",
    "        \\gamma^{new} (z_{mk}) \\mathbf{A}^{new} \\Big) \\\\ \n",
    "    &= \\mathbf{\\Sigma}_k^{old} + \\Big(\n",
    "        \\frac{\\gamma^{old}(z_{mk}) - \\gamma^{new}(z_{mk})}{N_k^{new}} \\big ) \\mathbf{\\Sigma}_k^{old} -\n",
    "        \\frac{\\gamma^{old} (z_{mk})}{N_k^{new}} \\mathbf{A}^{old} +\n",
    "        \\frac{\\gamma^{new} (z_{mk})}{N_k^{new}} \\mathbf{A}^{new} \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the mixing coefficients\n",
    "\n",
    "Update for mixing coefficients. $\\pi_{k} = \\frac{N_{k}}{N}$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\pi_{k}^{old} = \\frac{N_{k}^{old}}{N}\n",
    "\\end{equation}\n",
    "\n",
    "Update the responsibilities...\n",
    "\n",
    "\\begin{align}\n",
    "    \\pi_{k}^{new} &= \\frac{N_{k}^{new}}{N} \\\\\n",
    "        &= \\frac{1}{N} \\sum_{n \\neq m} \\gamma^{old}(z_{nk}) + \\gamma^{new}(z_{mk}) \\\\\n",
    "        &= \\frac{1}{N} \\Big( N_{k}^{old} + \\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk}) \\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm for Online Gaussian Mixture\n",
    "\n",
    "1. As usual, initalize the means $\\mathbf{\\mu}_k$, covariances $\\Sigma_k$, and mixing coefifients $\\pi_k$, and evaluate the inital value of the log likelihood. In addition, initalize the responsibilities, $\\gamma(z_{nk})$ to random numbers between 0 and 1. Use this to compute your initial $N_k$\n",
    "\n",
    "    \\begin{equation}\n",
    "        N_{k} = \\sum_{n=1}^{N} \\gamma(z_{nk}).\n",
    "    \\end{equation}\n",
    "    \n",
    "2. **E Step** Evaluate the responsibilities using the current parameter values for a single data point\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\gamma(z_{mk}) = \\frac{\\pi_{k} \n",
    "                                \\mathcal{N}( \\mathbf{x}_m \\mid \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\n",
    "                                }{\\Sigma_{j=1}^{K} \\pi_{j} \\mathcal{N}( \\mathbf{x}_m \\mid \\mathbf{\\mu}_j, \\mathbf{\\Sigma}_j)}\n",
    "    \\end{equation}\n",
    "\n",
    "3. **M Step** Update the parameters using the current responsibilities.\n",
    "\n",
    "    \\begin{align}\n",
    "        N_{k}^{new} &= N_{k}^{old} + \\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk}) \\\\\n",
    "        \\mathbf{\\mu}_k^{new} &= \\mathbf{\\mu}_k^{old} + \n",
    "            \\Big( \\frac{\\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})}{N_{k}^{new}} \\Big)\n",
    "            (\\mathbf{x}_m - \\mathbf{\\mu}_k^{old}) \\\\\n",
    "        \\mathbf{\\Sigma}_k^{new} &= \\mathbf{\\Sigma}_k^{old} +\n",
    "            \\Big( \\frac{\\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})}{N_{k}^{new}} \\Big)\n",
    "            \\Big( (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new}) (\\mathbf{x}_m - \\mathbf{\\mu}_k^{new})^{T} - \\mathbf{\\Sigma}_k^{old} \\Big) \\\\\n",
    "        \\pi_{k}^{new} &= \\frac{1}{N} \\Big( N_{k}^{old} + \\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk}) \\Big)\n",
    "    \\end{align}\n",
    "    \n",
    "4. Evaluate the log likelihood\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\ln p(\\mathbf{X} \\mid \\mathbf{\\mu}, \\mathbf{\\Sigma}, \\mathbf{\\pi}) =\n",
    "            \\sum_{n=1}^{N} \\ln \n",
    "            \\Big\\{ \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}(\\mathbf{x}_n \\mid \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_{k} \\Big\\}\n",
    "    \\end{equation}\n",
    "    \n",
    "    and check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and Example\n",
    "Note that the implementation was modified from code written by my classmate, Valentin Wolf.  \n",
    "https://github.com/volflow/Expectation-Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class EM:\n",
    "    def __init__(self, X, clusters=2, init_cov_size=120):\n",
    "        self.X = X\n",
    "        self.clusters = clusters\n",
    "        self.datapoints = self.X.shape[0]\n",
    "        dims = self.X.shape[1]\n",
    "        self.it = 0\n",
    "        self.init_cov_size = init_cov_size\n",
    "\n",
    "        # initialize with random points and identitiy matrices\n",
    "        # self.cluster_centers = np.random.uniform(low=self.X.min(axis=0),\n",
    "        #                                          high=self.X.max(axis=0),\n",
    "        #                                          size=(self.clusters,self.X.shape[1]))\n",
    "\n",
    "        # init means with random points from the data; seems to result in fewer singulartities\n",
    "        rand = np.random.choice(self.datapoints, self.clusters, replace=False)\n",
    "        self.cluster_centers = self.X[rand, :]\n",
    "\n",
    "        self.cluster_covs = np.stack([np.eye(dims) * self.init_cov_size] * self.clusters, axis=0)\n",
    "        self.mixing_coeffs = np.full(self.clusters, 1 / self.clusters)\n",
    "\n",
    "        self.responsibilities = np.random.rand(self.clusters, X.shape[0])\n",
    "\n",
    "    def fit(self, iterations=10):\n",
    "        for i in range(iterations):\n",
    "            online_dp_ind = self.it % self.X.shape[0]\n",
    "            # Expectation\n",
    "            new_responsibilities = self._expectation(self.X, online_dp_ind)\n",
    "            # Maximization\n",
    "            self._maximization(new_responsibilities, online_dp_ind)\n",
    "            self.responsibilities = new_responsibilities\n",
    "            self.it += 1\n",
    "        return self.cluster_centers, self.cluster_covs\n",
    "\n",
    "    def _expectation(self, X, online_dp_ind):\n",
    "        tripel = zip(self.cluster_centers, self.cluster_covs, self.mixing_coeffs)\n",
    "        responsibilities = self.responsibilities.copy()\n",
    "        divisor_sum = 0\n",
    "\n",
    "        for i, (mean, cov, mixing_coeff) in enumerate(tripel):\n",
    "            resp = mixing_coeff * multivariate_normal.pdf(X[online_dp_ind, :], mean, cov, allow_singular=True)\n",
    "            responsibilities[i, online_dp_ind] = resp\n",
    "            divisor_sum += resp\n",
    "        responsibilities[:, online_dp_ind] /= divisor_sum\n",
    "        return responsibilities\n",
    "\n",
    "    def _maximization(self, new_responsibilities, online_dp_ind):\n",
    "        X = self.X\n",
    "        for i, (new_resp, old_resp) in enumerate(zip(new_responsibilities, self.responsibilities)):\n",
    "            Nk_old = old_resp.sum()\n",
    "            mean_old = self.cluster_centers[i].copy()\n",
    "            cov_old = self.cluster_covs[i].copy()\n",
    "            mixcoef_old = self.mixing_coeffs[i].copy()\n",
    "            gamma_old = old_resp[online_dp_ind]\n",
    "\n",
    "            point = X[online_dp_ind, i]\n",
    "\n",
    "            gamma_new = new_resp[online_dp_ind]\n",
    "            Nk_new = Nk_old + gamma_new - gamma_old\n",
    "            if Nk_new <= 1:\n",
    "                # catch near singularities\n",
    "                print(\"Singularity detected\")\n",
    "\n",
    "                # choosing new mean uniformly random\n",
    "                # new_mean = np.random.uniform(low=self.X.min(axis=0),\n",
    "                #                              high=self.X.max(axis=0))\n",
    "\n",
    "                # choosing random points form X as mean\n",
    "                rand = np.random.choice(self.datapoints, self.clusters, replace=False)\n",
    "                mean_new = X[rand, :]\n",
    "                cov_new = np.eye(self.X.shape[1]) * self.init_cov_size\n",
    "            else:\n",
    "                mean_new = mean_old + ((gamma_new - gamma_old)/Nk_new) * (point - mean_old)\n",
    "                A_old = np.outer((point - mean_old), (point - mean_old).T)\n",
    "                A_new = np.outer((point - mean_new), (point - mean_new).T)\n",
    "                cov_new = cov_old + \\\n",
    "                          ((gamma_old - gamma_new)/Nk_new) * cov_old - \\\n",
    "                          gamma_old / Nk_new * A_old + \\\n",
    "                          gamma_new / Nk_new * A_new\n",
    "            mixcoef_new = Nk_new / self.responsibilities.shape[0]\n",
    "\n",
    "            self.cluster_centers[i] = mean_new\n",
    "            self.cluster_covs[i] = cov_new\n",
    "            self.mixing_coeffs[i] = mixcoef_new\n",
    "\n",
    "    def predict(self, X):\n",
    "        resp = self._expectation(X)\n",
    "        cluster_prediction = resp.argmax(axis=0)\n",
    "        prediction = np.copy(X)\n",
    "        for i, mean in enumerate(self.cluster_centers):\n",
    "            prediction[cluster_prediction == i] = mean\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import pi, sin, cos\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def oval(cov, num_points=100,radius=1):\n",
    "    arcs = np.linspace(0, 2 * pi, num_points)\n",
    "    x = radius * sin(arcs)\n",
    "    y = radius * cos(arcs)\n",
    "    \n",
    "    xy = np.array(list(zip(x, y)))\n",
    "    x, y = zip(*xy.dot(cov))\n",
    "    return x,y\n",
    "\n",
    "def make_plot(a):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(\"EM iteration {}\".format(a.it))\n",
    "\n",
    "    colors = ['g', 'r', 'c', 'm', 'y', 'b' ]\n",
    "    \n",
    "    # selcect elements based on expectation\n",
    "    x, y = zip(*X)\n",
    "    try:\n",
    "        plt.scatter(x, y, edgecolors=\"black\",c=a.responsibilities[0],cmap='RdYlGn')\n",
    "    except AttributeError:\n",
    "        plt.scatter(x, y, edgecolors=\"black\",color='y')\n",
    "    for i in range(a.cluster_centers.shape[0]):\n",
    "        # plot centers\n",
    "        plt.scatter(a.cluster_centers[i,0],a.cluster_centers[i,1],s=250,color=colors[i],edgecolors=\"white\")\n",
    "\n",
    "        # plot ovals that show the shape of the  variances\n",
    "        x, y = oval(a.cluster_covs[i],radius=2)\n",
    "        x += a.cluster_centers[i,0]\n",
    "        y += a.cluster_centers[i,1]\n",
    "        plt.plot(x, y,linewidth=5,color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with 2D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "X = pd.read_csv(\"data/2d-em.csv\", header=None).as_matrix()\n",
    "# plot dataset\n",
    "x, y = zip(*X)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(x, y, edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = EM(X,2,init_cov_size=2)\n",
    "for i in [1, 100, 100, 100]: \n",
    "    make_plot(a)\n",
    "    mm = a.fit(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
